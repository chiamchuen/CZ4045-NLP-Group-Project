{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\potat\\anaconda3\\envs\\nnb1\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\potat\\anaconda3\\envs\\nnb1\\lib\\site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\potat\\anaconda3\\envs\\nnb1\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\potat\\anaconda3\\envs\\nnb1\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.1\n",
    "Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar\n",
    "word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word\n",
    "and its cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('students', 0.7294867038726807)]\n",
      "[('Apple_AAPL', 0.7456986308097839)]\n",
      "[('apples', 0.720359742641449)]\n"
     ]
    }
   ],
   "source": [
    "similar_to_student = word2vec.most_similar('student', topn=1)\n",
    "print(similar_to_student)\n",
    "similar_to_Apple = word2vec.most_similar('Apple', topn=1)\n",
    "print(similar_to_Apple)\n",
    "similar_to_apple = word2vec.most_similar('apple', topn=1)\n",
    "print(similar_to_apple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.2\n",
    "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training files: 14987\n",
      "Size of development files: 3466\n",
      "Size of test files: 3684\n",
      "Include last marker or not: -DOCSTART- -X- O O\n",
      "\n",
      "Labels: ['B-LOC', 'B-MISC', 'I-LOC', 'O', 'I-PER', 'B-ORG', 'I-MISC', 'I-ORG']\n"
     ]
    }
   ],
   "source": [
    "with open('eng.train', 'r') as train_file:\n",
    "    train_data = train_file.read()\n",
    "\n",
    "with open('eng.testa', 'r') as testa_file:\n",
    "    dev_data = testa_file.read()\n",
    "\n",
    "with open('eng.testb', 'r') as testb_file:\n",
    "    test_data = testb_file.read()\n",
    "\n",
    "train_sentences = train_data.split('\\n\\n')\n",
    "print(\"Size of training files:\", len(train_sentences))\n",
    "\n",
    "dev_sentences = dev_data.split('\\n\\n')\n",
    "print(\"Size of development files:\", len(dev_sentences))\n",
    "\n",
    "test_sentences = test_data.split('\\n\\n')\n",
    "print(\"Size of test files:\", len(test_sentences))\n",
    "\n",
    "print(\"Include last marker or not:\", train_sentences[-1])\n",
    "\n",
    "label_set = set()\n",
    "for sentence in train_sentences:\n",
    "    lines = sentence.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.strip(): \n",
    "            parts = line.split()\n",
    "            label = parts[-1]\n",
    "            label_set.add(label)\n",
    "\n",
    "label_list = list(label_set)\n",
    "print(\"Labels:\", label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" O O\n",
      "Rajavi NNP I-NP I-MISC\n",
      "emphasised VBD I-VP O\n",
      "that IN I-SBAR O\n",
      "the DT I-NP O\n",
      "Iranian JJ I-NP I-MISC\n",
      "Resistance NN I-NP I-ORG\n",
      "would MD I-VP O\n",
      "continue VB I-VP O\n",
      "to TO I-VP O\n",
      "stand VB I-VP O\n",
      "side NN I-NP O\n",
      "by IN I-PP O\n",
      "side NN I-NP O\n",
      "with IN I-PP O\n",
      "their PRP$ I-NP O\n",
      "Kurdish JJ I-NP I-MISC\n",
      "compatriots NNS I-NP O\n",
      "and CC O O\n",
      "the DT I-NP O\n",
      "resistance NN I-NP O\n",
      "movement NN I-NP O\n",
      "in IN I-PP O\n",
      "Iranian NNP I-NP I-LOC\n",
      "Kurdistan NNP I-NP I-LOC\n",
      ", , O O\n",
      "\" \" O O\n",
      "it PRP I-NP O\n",
      "said VBD I-VP O\n",
      ". . O O\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['B-LOC', 'B-MISC', 'I-LOC', 'O', 'I-PER', 'B-ORG', 'I-MISC', 'I-ORG']\n",
      "EU NNP I-NP I-ORG\n",
      "rejects VBZ I-VP O\n",
      "German JJ I-NP I-MISC\n",
      "call NN I-NP O\n",
      "to TO I-VP O\n",
      "boycott VB I-VP O\n",
      "British JJ I-NP I-MISC\n",
      "lamb NN I-NP O\n",
      ". . O O\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rejects</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>call</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boycott</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>British</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lamb</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  label\n",
       "0       EU      7\n",
       "1  rejects      3\n",
       "2   German      6\n",
       "3     call      3\n",
       "4       to      3\n",
       "5  boycott      3\n",
       "6  British      6\n",
       "7     lamb      3\n",
       "8        .      3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_label(label):\n",
    "    return label_list.index(label)\n",
    "\n",
    "train_pairs = [\n",
    "    (line.split()[0], convert_label(line.split()[-1].split()[-1]))\n",
    "    for sentence in train_sentences\n",
    "    for line in sentence.split('\\n')\n",
    "    if line.strip()\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(train_pairs, columns=['word', 'label'])\n",
    "\n",
    "dev_pairs = [\n",
    "    (line.split()[0], convert_label(line.split()[-1].split()[-1]))\n",
    "    for sentence in dev_sentences\n",
    "    for line in sentence.split('\\n')\n",
    "    if line.strip()\n",
    "]\n",
    "\n",
    "dev_df = pd.DataFrame(dev_pairs, columns=['word', 'label'])\n",
    "\n",
    "test_pairs = [\n",
    "    (line.split()[0], convert_label(line.split()[-1].split()[-1]))\n",
    "    for sentence in test_sentences\n",
    "    for line in sentence.split('\\n')\n",
    "    if line.strip()\n",
    "]\n",
    "\n",
    "test_df = pd.DataFrame(test_pairs, columns=['word', 'label'])\n",
    "print(\"Labels:\", label_list)\n",
    "print(train_sentences[0])\n",
    "train_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "embedding_dim = word2vec.vector_size\n",
    "np.random.seed(0)\n",
    "unknown = np.random.rand(embedding_dim)\n",
    "\n",
    "class NER(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size, word2vec):\n",
    "        super(NER, self).__init__()\n",
    "        self.word2vec = word2vec\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.reLU = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.outputLayer = nn.Linear(2 * hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, word):\n",
    "        word_embeddings = [self.word2vec[word] if word in self.word2vec else unknown for word in word]\n",
    "        word_embeddings_array = np.stack(word_embeddings)\n",
    "        word_vectors = torch.tensor(word_embeddings_array).to(dtype=torch.float32)\n",
    "        lstm_output, _ = self.lstm(word_vectors)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        output = self.outputLayer(lstm_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.data.iloc[idx]['word']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        return word, torch.tensor(label)\n",
    "    \n",
    "train_dataset = NERDataset(train_df)\n",
    "test_dataset = NERDataset(dev_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NER(embedding_dim=embedding_dim, hidden_dim=150, output_size=8, word2vec=word2vec)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1649.9436, F1 Dev=0.9736\n",
      "Epoch 2: Loss=904.2576, F1 Dev=0.9779\n",
      "Epoch 3: Loss=590.1367, F1 Dev=0.9787\n",
      "Epoch 4: Loss=393.8825, F1 Dev=0.9788\n",
      "Epoch 5: Loss=290.0817, F1 Dev=0.9784\n",
      "Epoch 6: Loss=224.3826, F1 Dev=0.9781\n",
      "Epoch 7: Loss=188.8434, F1 Dev=0.9785\n",
      "Epoch 8: Loss=160.5177, F1 Dev=0.9795\n",
      "Epoch 9: Loss=145.1345, F1 Dev=0.9788\n",
      "Epoch 10: Loss=131.1922, F1 Dev=0.9784\n",
      "Epoch 11: Loss=113.5317, F1 Dev=0.9789\n",
      "Epoch 12: Loss=112.5197, F1 Dev=0.9792\n",
      "Epoch 13: Loss=94.4065, F1 Dev=0.9791\n",
      "No improvement for 5 epochs. Training stopped.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_score(predictions, labels):\n",
    "    return f1_score(labels, predictions, average='micro')\n",
    "\n",
    "best_f1_score = 0.0\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        words, labels = batch\n",
    "        tag_score = model(words)\n",
    "        loss = loss_fn(tag_score.view(-1, model.outputLayer.out_features), labels.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval() \n",
    "    dev_predictions = []\n",
    "    dev_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            words, labels = batch\n",
    "            tag_score = model(words)\n",
    "            _, predicted = torch.max(tag_score, 1)\n",
    "            dev_predictions.extend(predicted.view(-1).tolist())\n",
    "            dev_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "    f1_dev = calculate_f1_score(dev_predictions, dev_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss={total_loss:.4f}, F1 Dev={f1_dev:.4f}\")\n",
    "\n",
    "    if f1_dev > best_f1_score:\n",
    "        best_f1_score = f1_dev\n",
    "        best_model_state = model.state_dict()\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    if no_improvement >= patience:\n",
    "        print(f\"No improvement for {patience} epochs. Training stopped.\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([3, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 7, 7, 3, 3])\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_output = model([\"This\", \"is\", \"CZ4045\", \"NLP\", \"group\", \"project\", \"from\", \"NTU\", \"'s\", \"School\", \"of\", \"Computer\", \"Science\", \"and\", \"Engineering\" \".\"])\n",
    "_, predicted = torch.max(test_output, 1)\n",
    "\n",
    "predicted_labels = [label_list[i] for i in predicted.tolist()]\n",
    "\n",
    "print(\"Predicted:\", predicted)\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Discuss how you deal with new words in the training set which are not found in the pretrained\n",
    "dictionary. Likewise, how do you deal with new words in the test set which are not found in\n",
    "either the pretrained dictionary or the training set? Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Describe what neural network you used to produce the final vector representation of each\n",
    "word and what are the mathematical functions used for the forward computation (i.e., from\n",
    "the pretrained word vectors to the final label of each word). Give the detailed setting of the\n",
    "network including which parameters are being updated, what are their sizes, and what is the\n",
    "length of the final vector representation of each word to be fed to the softmax classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Report how many epochs you used for training, as well as the running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Report the f1 score on the test set, as well as the f1 score on the development set for each\n",
    "epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnb1",
   "language": "python",
   "name": "nnb1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
